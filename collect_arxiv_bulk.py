"""
arXiv æ‰¹é‡å†å²æ•°æ®é‡‡é›†è„šæœ¬

åŠŸèƒ½ï¼š
1. æŒ‰å¹´ä»½æ‰¹é‡é‡‡é›†arXivè®ºæ–‡
2. æ”¯æŒ2020-2025å¹´çš„å†å²æ•°æ®
3. æ¯å¹´é‡‡é›†2000ç¯‡ï¼Œæ€»è®¡12000+ç¯‡
4. è‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
5. æ”¯æŒæ–­ç‚¹ç»­ä¼ 

ä½¿ç”¨æ–¹æ³•ï¼š
    python collect_arxiv_bulk.py --start-year 2020 --end-year 2025
    python collect_arxiv_bulk.py --start-year 2024 --end-year 2024 --per-year 3000
"""

import sys
import time
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from scraper.arxiv_client import create_arxiv_client, DEFAULT_CATEGORIES
from database import get_raw_repository


def collect_by_year(
    year: int,
    categories: list,
    max_per_year: int = 2000,
    client = None,
    repo = None
):
    """
    é‡‡é›†æŒ‡å®šå¹´ä»½çš„è®ºæ–‡

    Args:
        year: å¹´ä»½
        categories: åˆ†ç±»åˆ—è¡¨
        max_per_year: æ¯å¹´æœ€å¤§é‡‡é›†æ•°é‡
        client: arXivå®¢æˆ·ç«¯
        repo: æ•°æ®åº“ä»“åº“

    Returns:
        é‡‡é›†ç»Ÿè®¡
    """
    print(f"\n{'='*70}")
    print(f"ğŸ“… é‡‡é›† {year} å¹´è®ºæ–‡")
    print(f"{'='*70}")

    # æŒ‰åˆ†ç±»é‡‡é›†
    all_papers = []
    per_category = max_per_year // len(categories)

    for category in categories:
        print(f"\nğŸ” é‡‡é›† {category} åˆ†ç±»...")

        # æ„å»ºå¹´ä»½æŸ¥è¯¢
        # arXiv APIä¸ç›´æ¥æ”¯æŒå¹´ä»½è¿‡æ»¤ï¼Œéœ€è¦é€šè¿‡æ—¥æœŸèŒƒå›´
        papers = client.search(
            categories=[category],
            max_results=per_category
        )

        # è¿‡æ»¤å¹´ä»½
        year_papers = [p for p in papers if p.year == year]
        all_papers.extend(year_papers)

        print(f"   âœ… {category}: è·å– {len(year_papers)} ç¯‡")

        # é¿å…APIé™åˆ¶
        time.sleep(5)

    print(f"\nğŸ“Š {year} å¹´æ€»è®¡: {len(all_papers)} ç¯‡")

    # ä¿å­˜åˆ°æ•°æ®åº“
    print(f"ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
    saved = 0
    duplicates = 0
    errors = 0

    for paper in all_papers:
        try:
            repo.save_raw_paper(paper)
            saved += 1
        except Exception as e:
            if "UNIQUE constraint failed" in str(e):
                duplicates += 1
            else:
                errors += 1

    print(f"âœ… ä¿å­˜å®Œæˆ: æ–°å¢ {saved}, é‡å¤ {duplicates}, å¤±è´¥ {errors}")

    return {
        "year": year,
        "fetched": len(all_papers),
        "saved": saved,
        "duplicates": duplicates,
        "errors": errors
    }


def collect_bulk(
    start_year: int = 2020,
    end_year: int = 2025,
    categories: list = None,
    per_year: int = 2000,
    verbose: bool = True
):
    """
    æ‰¹é‡é‡‡é›†å¤šå¹´æ•°æ®

    Args:
        start_year: èµ·å§‹å¹´ä»½
        end_year: ç»“æŸå¹´ä»½
        categories: åˆ†ç±»åˆ—è¡¨
        per_year: æ¯å¹´é‡‡é›†æ•°é‡
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        æ€»ä½“ç»Ÿè®¡
    """
    client = create_arxiv_client(delay=3.0)
    repo = get_raw_repository()
    categories = categories or DEFAULT_CATEGORIES

    print("=" * 70)
    print("ğŸ“¥ arXiv æ‰¹é‡å†å²æ•°æ®é‡‡é›†")
    print("=" * 70)
    print(f"ğŸ“… å¹´ä»½èŒƒå›´: {start_year} - {end_year}")
    print(f"ğŸ“š åˆ†ç±»: {', '.join(categories)}")
    print(f"ğŸ¯ æ¯å¹´ç›®æ ‡: {per_year} ç¯‡")
    print(f"ğŸ“Š é¢„è®¡æ€»é‡: {(end_year - start_year + 1) * per_year} ç¯‡")
    print(f"â±ï¸  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)

    # æŒ‰å¹´ä»½é‡‡é›†
    all_stats = []
    total_fetched = 0
    total_saved = 0

    for year in range(start_year, end_year + 1):
        try:
            stats = collect_by_year(
                year=year,
                categories=categories,
                max_per_year=per_year,
                client=client,
                repo=repo
            )

            all_stats.append(stats)
            total_fetched += stats["fetched"]
            total_saved += stats["saved"]

            # ä¼‘æ¯é¿å…APIé™åˆ¶
            print(f"\nâ¸ï¸  ä¼‘æ¯10ç§’...")
            time.sleep(10)

        except KeyboardInterrupt:
            print(f"\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­é‡‡é›†")
            break
        except Exception as e:
            print(f"\nâŒ {year} å¹´é‡‡é›†å¤±è´¥: {e}")
            continue

    # æ€»ä½“ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š æ‰¹é‡é‡‡é›†æ€»ç»“")
    print("=" * 70)
    print(f"âœ… å®Œæˆå¹´ä»½: {len(all_stats)}/{end_year - start_year + 1}")
    print(f"ğŸ“¥ æ€»è·å–: {total_fetched} ç¯‡")
    print(f"ğŸ’¾ æ€»ä¿å­˜: {total_saved} ç¯‡")
    print(f"â±ï¸  å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nå¹´åº¦æ˜ç»†:")
    print("-" * 70)

    for stats in all_stats:
        print(f"  {stats['year']}: è·å– {stats['fetched']:4d} | "
              f"ä¿å­˜ {stats['saved']:4d} | "
              f"é‡å¤ {stats['duplicates']:3d} | "
              f"å¤±è´¥ {stats['errors']:2d}")

    print("=" * 70)

    # æ•°æ®åº“ç»Ÿè®¡
    total_in_db = repo.count_raw_papers_by_source("arxiv")
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"   arXivè®ºæ–‡æ€»æ•°: {total_in_db} ç¯‡")

    return {
        "years_completed": len(all_stats),
        "total_fetched": total_fetched,
        "total_saved": total_saved,
        "stats_by_year": all_stats
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡é‡‡é›†arXivå†å²æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é‡‡é›†2020-2025å¹´çš„æ‰€æœ‰æ•°æ®
  python collect_arxiv_bulk.py --start-year 2020 --end-year 2025

  # åªé‡‡é›†2024å¹´çš„æ•°æ®
  python collect_arxiv_bulk.py --start-year 2024 --end-year 2024

  # é‡‡é›†2022-2023å¹´ï¼Œæ¯å¹´3000ç¯‡
  python collect_arxiv_bulk.py --start-year 2022 --end-year 2023 --per-year 3000

  # åªé‡‡é›†cs.LGå’Œcs.CVåˆ†ç±»
  python collect_arxiv_bulk.py --categories cs.LG cs.CV
        """
    )

    parser.add_argument(
        "--start-year",
        type=int,
        default=2020,
        help="èµ·å§‹å¹´ä»½ (é»˜è®¤: 2020)"
    )

    parser.add_argument(
        "--end-year",
        type=int,
        default=2025,
        help="ç»“æŸå¹´ä»½ (é»˜è®¤: 2025)"
    )

    parser.add_argument(
        "--categories",
        nargs="+",
        default=None,
        help=f"arXivåˆ†ç±»åˆ—è¡¨ (é»˜è®¤: {' '.join(DEFAULT_CATEGORIES)})"
    )

    parser.add_argument(
        "--per-year",
        type=int,
        default=2000,
        help="æ¯å¹´é‡‡é›†æ•°é‡ (é»˜è®¤: 2000)"
    )

    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼"
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.start_year > args.end_year:
        print("âŒ é”™è¯¯: èµ·å§‹å¹´ä»½ä¸èƒ½å¤§äºç»“æŸå¹´ä»½")
        sys.exit(1)

    if args.start_year < 1990 or args.end_year > datetime.now().year + 1:
        print(f"âŒ é”™è¯¯: å¹´ä»½èŒƒå›´åº”åœ¨ 1990 - {datetime.now().year + 1} ä¹‹é—´")
        sys.exit(1)

    # æ‰§è¡Œé‡‡é›†
    try:
        stats = collect_bulk(
            start_year=args.start_year,
            end_year=args.end_year,
            categories=args.categories,
            per_year=args.per_year,
            verbose=not args.quiet
        )

        # è¿”å›çŠ¶æ€ç 
        if stats["total_saved"] > 0:
            sys.exit(0)
        else:
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­é‡‡é›†")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nâŒ é‡‡é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
