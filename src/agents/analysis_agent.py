"""
Analysis Agent

è´Ÿè´£ä» Structured Layer æå–å…³é”®è¯åˆ° Analysis Layerã€‚

èŒè´£ï¼š
- å¢é‡å¤„ç†ï¼šä»…å¤„ç†æœªæå–å…³é”®è¯çš„è®ºæ–‡
- å…³é”®è¯æå–ï¼šYAKEï¼ˆå¿«é€Ÿï¼‰+ KeyBERTï¼ˆç²¾å‡†ï¼‰
- å…³é”®è¯è§„èŒƒåŒ–ï¼šç»Ÿä¸€æ ¼å¼ã€å»é‡
- ä¿å­˜åˆ° paper_keywords è¡¨
"""

import sys
import re
from pathlib import Path
from typing import List, Optional, Dict, Tuple, Literal
from datetime import datetime

# ç¡®ä¿ src ç›®å½•åœ¨è·¯å¾„ä¸­
_src_dir = Path(__file__).parent.parent
if str(_src_dir) not in sys.path:
    sys.path.insert(0, str(_src_dir))

from database import get_structured_repository, get_analysis_repository
from database.repository import StructuredRepository, AnalysisRepository
from scraper.models import Paper, PaperKeyword
from extractor.yake_extractor import YakeExtractor, create_yake_extractor
from extractor.keybert_extractor import KeyBertExtractor, get_keybert_extractor


ExtractorType = Literal["yake", "keybert", "both"]


class AnalysisAgent:
    """
    åˆ†æ Agent
    
    è´Ÿè´£ä» papers è¡¨æå–å…³é”®è¯åˆ° paper_keywords è¡¨ã€‚
    """
    
    def __init__(
        self,
        structured_repo: StructuredRepository = None,
        analysis_repo: AnalysisRepository = None,
        yake_extractor: YakeExtractor = None,
        keybert_extractor: KeyBertExtractor = None,
    ):
        self.structured_repo = structured_repo or get_structured_repository()
        self.analysis_repo = analysis_repo or get_analysis_repository()
        self._yake = yake_extractor
        self._keybert = keybert_extractor
    
    def _get_yake(self) -> YakeExtractor:
        """æ‡’åŠ è½½ YAKE æå–å™¨"""
        if self._yake is None:
            self._yake = create_yake_extractor()
        return self._yake
    
    def _get_keybert(self) -> KeyBertExtractor:
        """æ‡’åŠ è½½ KeyBERT æå–å™¨"""
        if self._keybert is None:
            self._keybert = get_keybert_extractor()
        return self._keybert
    
    # ========== Step 1: å¢é‡é€‰æ‹© ==========
    
    def get_papers_without_keywords(
        self,
        method: str = "yake",
        limit: int = 1000,
    ) -> List[Paper]:
        """
        è·å–è¿˜æ²¡æœ‰æå–å…³é”®è¯çš„è®ºæ–‡ï¼ˆå¢é‡ï¼‰
        
        Args:
            method: æå–æ–¹æ³•ï¼ˆyake/keybertï¼‰
            limit: æœ€å¤§æ•°é‡
            
        Returns:
            éœ€è¦å¤„ç†çš„è®ºæ–‡åˆ—è¡¨
        """
        return self.analysis_repo.get_papers_without_keywords(method=method, limit=limit)
    
    # ========== Step 2: æ„å»ºæå–æ–‡æœ¬ ==========
    
    def get_text_for_extraction(self, paper: Paper) -> str:
        """
        è·å–ç”¨äºå…³é”®è¯æå–çš„æ–‡æœ¬
        
        ä½¿ç”¨ Paper.text_for_extraction: "{canonical_title}. {abstract}"
        """
        return paper.text_for_extraction
    
    # ========== Step 3: å…³é”®è¯æå– ==========
    
    def extract_keywords_yake(
        self,
        text: str,
        top_n: int = 10,
    ) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ YAKE æå–å…³é”®è¯
        
        Returns:
            [(keyword, score), ...] score è¶Šé«˜è¶Šå¥½
        """
        extractor = self._get_yake()
        # YAKE çš„ num_keywords åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®ï¼Œè¿™é‡Œç›´æ¥è°ƒç”¨
        results = extractor.extract(text)
        return results[:top_n]  # æˆªå– top_n
    
    def extract_keywords_keybert(
        self,
        text: str,
        top_n: int = 10,
    ) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ KeyBERT æå–å…³é”®è¯
        
        Returns:
            [(keyword, score), ...]
        """
        extractor = self._get_keybert()
        # KeyBERT çš„æ¥å£å¯èƒ½ä¹Ÿç±»ä¼¼
        results = extractor.extract(text)
        return results[:top_n]
    
    # ========== è§„èŒƒåŒ–ä¸è¿‡æ»¤ ==========
    
    def _get_filter(self):
        """è·å–å…³é”®è¯è¿‡æ»¤å™¨"""
        if not hasattr(self, '_filter'):
            from extractor.keyword_filter import get_keyword_filter
            self._filter = get_keyword_filter()
        return self._filter
    
    def filter_keywords(
        self,
        keywords: List[Tuple[str, float]],
        fuzzy_dedup: bool = True,
    ) -> List[Tuple[str, float]]:
        """
        è¿‡æ»¤å¹¶è§„èŒƒåŒ–å…³é”®è¯
        
        åŒ…å«ï¼š
        - è§„èŒƒåŒ–ï¼ˆlower, æ ‡ç‚¹, ç©ºç™½ï¼‰
        - è¿‡æ»¤ï¼ˆbanned, stopwords, noiseï¼‰
        - åŒä¹‰å½’å¹¶
        - å»é‡ï¼ˆä¸¥æ ¼ + è¿‘ä¼¼ï¼‰
        """
        return self._get_filter().process(keywords, fuzzy_dedup=fuzzy_dedup)
    
    # ========== å¤„ç†æµç¨‹ ==========
    
    def process_paper(
        self,
        paper: Paper,
        method: str = "yake",
        top_n: int = 15,  # æå–æ›´å¤šï¼Œè¿‡æ»¤åä¿ç•™ ~10
    ) -> List[PaperKeyword]:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼Œæå–å¹¶è¿‡æ»¤å…³é”®è¯
        
        Returns:
            PaperKeyword åˆ—è¡¨
        """
        text = self.get_text_for_extraction(paper)
        if not text or len(text) < 10:
            return []
        
        # æå–å…³é”®è¯ï¼ˆå¤šæå–ä¸€äº›ï¼Œåç»­è¿‡æ»¤ï¼‰
        if method == "yake":
            raw_keywords = self.extract_keywords_yake(text, top_n=top_n)
        elif method == "keybert":
            raw_keywords = self.extract_keywords_keybert(text, top_n=top_n)
        else:
            raise ValueError(f"æœªçŸ¥çš„æå–æ–¹æ³•: {method}")
        
        # è¿‡æ»¤ã€è§„èŒƒåŒ–ã€å»é‡ã€åŒä¹‰å½’å¹¶
        filtered = self.filter_keywords(raw_keywords, fuzzy_dedup=True)
        
        # æˆªå–æœ€ç»ˆä¿ç•™æ•°é‡
        filtered = filtered[:10]
        
        # æ„å»º PaperKeyword å¯¹è±¡
        results = []
        for kw, score in filtered:
            results.append(PaperKeyword(
                paper_id=paper.paper_id,
                keyword=kw,
                method=method,
                score=score,
            ))
        
        return results
    
    def run(
        self,
        method: str = "yake",
        limit: int = 1000,
        top_n: int = 10,
        force: bool = False,
    ) -> Dict[str, int]:
        """
        è¿è¡Œå¢é‡åˆ†ææµç¨‹
        
        Args:
            method: æå–æ–¹æ³•
            limit: å•æ¬¡å¤„ç†ä¸Šé™
            top_n: æ¯ç¯‡è®ºæ–‡æå–çš„å…³é”®è¯æ•°
            force: å¼ºåˆ¶è¿è¡Œï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            å¤„ç†ç»Ÿè®¡
        """
        print("\n" + "=" * 60)
        print(f"ğŸ”‘ [Analysis Agent] å¼€å§‹å…³é”®è¯æå– (method={method})")
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œåˆ†æ
        if not force and not self.should_run_analysis():
            print("   â­ï¸ æ— æ–°æ•°æ®ï¼Œè·³è¿‡åˆ†æ")
            return {"status": "skipped", "reason": "no_new_data", "processed": 0, "keywords": 0}
        
        # Step 1: è·å–å¾…å¤„ç†è®ºæ–‡
        papers = self.get_papers_without_keywords(method=method, limit=limit)
        
        if not papers:
            print("   âœ… æ²¡æœ‰éœ€è¦å¤„ç†çš„è®ºæ–‡ï¼ˆå·²å…¨éƒ¨æå–ï¼‰")
            # ä»ç„¶æ›´æ–°å…ƒä¿¡æ¯ï¼Œè¡¨ç¤ºå·²æ£€æŸ¥
            self._update_analysis_meta()
            return {"processed": 0, "keywords": 0}
        
        print(f"\nğŸ“ å¾…å¤„ç†è®ºæ–‡: {len(papers)} ç¯‡")
        
        # Step 2-3: å¤„ç†æ¯ç¯‡è®ºæ–‡
        total_keywords = 0
        processed = 0
        
        for i, paper in enumerate(papers):
            try:
                keywords = self.process_paper(paper, method=method, top_n=top_n)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                for pk in keywords:
                    self.analysis_repo.save_paper_keyword(pk)
                
                total_keywords += len(keywords)
                processed += 1
                
                if (i + 1) % 100 == 0:
                    print(f"   å·²å¤„ç† {i + 1}/{len(papers)}...")
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥ (paper_id={paper.paper_id}): {e}")
        
        # æ›´æ–°å…ƒä¿¡æ¯
        self._update_analysis_meta()
        
        # æ›´æ–°ä¼šè®®ç¼“å­˜
        self._update_venue_caches()
        
        print(f"\nâœ… [Analysis] å¤„ç†å®Œæˆ")
        print(f"   - è®ºæ–‡æ•°: {processed}")
        print(f"   - å…³é”®è¯æ•°: {total_keywords}")
        print(f"   - å¹³å‡æ¯ç¯‡: {total_keywords / processed:.1f}" if processed else "")
        
        return {
            "processed": processed,
            "keywords": total_keywords,
        }
    
    def should_run_analysis(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦è¿è¡Œåˆ†æ
        
        æ£€æŸ¥ï¼š
        - raw_max_retrieved_at > analysis_meta.last_raw_max_retrieved_at
        - papers æ•°é‡æ˜¯å¦å˜åŒ–
        """
        # è·å–ä¸Šæ¬¡åˆ†æçš„å…ƒä¿¡æ¯
        last_retrieved = self.analysis_repo.get_meta("last_raw_max_retrieved_at")
        last_paper_count = self.analysis_repo.get_meta("last_paper_count")
        
        # è·å–å½“å‰çŠ¶æ€
        current_retrieved = self.analysis_repo.get_max_retrieved_at()
        current_paper_count = self.analysis_repo.get_total_paper_count()
        
        # é¦–æ¬¡è¿è¡Œ
        if last_retrieved is None:
            return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
        if current_retrieved and current_retrieved > last_retrieved:
            return True
        
        # æ£€æŸ¥è®ºæ–‡æ•°æ˜¯å¦å˜åŒ–
        if last_paper_count and str(current_paper_count) != last_paper_count:
            return True
        
        return False
    
    def _update_analysis_meta(self):
        """æ›´æ–°åˆ†æå…ƒä¿¡æ¯"""
        current_retrieved = self.analysis_repo.get_max_retrieved_at()
        current_paper_count = self.analysis_repo.get_total_paper_count()
        
        if current_retrieved:
            self.analysis_repo.set_meta("last_raw_max_retrieved_at", current_retrieved)
        
        self.analysis_repo.set_meta("last_paper_count", str(current_paper_count))
        self.analysis_repo.set_meta("last_analysis_run", datetime.now().isoformat())
    
    def _update_venue_caches(self):
        """æ›´æ–°ä¼šè®®æ€»è§ˆç¼“å­˜"""
        print("\nğŸ“Š æ›´æ–°ä¼šè®®ç¼“å­˜...")
        
        venues = self.structured_repo.get_all_venues()
        
        for venue in venues:
            venue_name = venue.canonical_name
            
            # è·å–è¯¥ä¼šè®®çš„ç»Ÿè®¡ä¿¡æ¯
            paper_count = self.structured_repo.get_paper_count(venue_id=venue.venue_id)
            top_keywords = self.analysis_repo.get_top_keywords(
                venue_id=venue.venue_id, 
                limit=20
            )
            
            # è½¬æ¢ä¸º JSON æ ¼å¼
            top_kw_list = [{"keyword": kw, "count": count} for kw, count in top_keywords]
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.analysis_repo.save_venue_summary(
                venue=venue_name,
                year=None,  # å…¨é‡ç»Ÿè®¡
                paper_count=paper_count,
                top_keywords=top_kw_list
            )
        
        print(f"   âœ… å·²æ›´æ–° {len(venues)} ä¸ªä¼šè®®çš„ç¼“å­˜")


def run_analysis(
    method: str = "yake",
    limit: int = 1000,
) -> Dict[str, int]:
    """è¿è¡Œåˆ†æçš„ä¾¿æ·å‡½æ•°"""
    agent = AnalysisAgent()
    return agent.run(method=method, limit=limit)
