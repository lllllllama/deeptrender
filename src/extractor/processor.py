"""
å…³é”®è¯å¤„ç†å™¨

æ‰¹é‡å¤„ç†è®ºæ–‡ï¼Œæå–å…³é”®è¯å¹¶è¿›è¡Œæ ‡å‡†åŒ–ã€‚
"""

from typing import List, Literal, Optional, Union
from tqdm import tqdm

from .yake_extractor import YakeExtractor, create_yake_extractor
from .keybert_extractor import KeyBertExtractor, get_keybert_extractor
from scraper.models import Paper
from config import EXTRACTOR_CONFIG


ExtractorType = Literal["yake", "keybert", "both"]


class KeywordProcessor:
    """å…³é”®è¯å¤„ç†å™¨"""
    
    def __init__(
        self,
        extractor_type: ExtractorType = None,
        yake_extractor: Optional[YakeExtractor] = None,
        keybert_extractor: Optional[KeyBertExtractor] = None,
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            extractor_type: æå–å™¨ç±»å‹ï¼ˆ"yake", "keybert", "both"ï¼‰
            yake_extractor: YAKE æå–å™¨å®ä¾‹
            keybert_extractor: KeyBERT æå–å™¨å®ä¾‹
        """
        self.extractor_type = extractor_type or EXTRACTOR_CONFIG.default_extractor
        
        # åˆå§‹åŒ–æå–å™¨
        if self.extractor_type in ("yake", "both"):
            self.yake = yake_extractor or create_yake_extractor()
        else:
            self.yake = None
        
        if self.extractor_type in ("keybert", "both"):
            self.keybert = keybert_extractor or get_keybert_extractor()
        else:
            self.keybert = None
    
    def extract_from_text(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        keywords = set()
        
        if self.yake:
            yake_keywords = self.yake.extract_keywords(text)
            keywords.update(yake_keywords)
        
        if self.keybert:
            keybert_keywords = self.keybert.extract_keywords(text)
            keywords.update(keybert_keywords)
        
        return list(keywords)
    
    def process_paper(self, paper: Paper) -> Paper:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼Œæå–å…³é”®è¯
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            
        Returns:
            æ›´æ–°äº† extracted_keywords çš„è®ºæ–‡å¯¹è±¡
        """
        text = paper.text_for_extraction
        extracted = self.extract_from_text(text)
        
        # æ ‡å‡†åŒ–å…³é”®è¯
        normalized = self._normalize_keywords(extracted)
        
        paper.extracted_keywords = normalized
        return paper
    
    def process_papers(
        self,
        papers: List[Paper],
        show_progress: bool = True,
    ) -> List[Paper]:
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            å¤„ç†åçš„è®ºæ–‡åˆ—è¡¨
        """
        print(f"\nğŸ”‘ æ­£åœ¨æå–å…³é”®è¯ï¼ˆä½¿ç”¨ {self.extractor_type}ï¼‰...")
        
        if show_progress:
            papers = tqdm(papers, desc="æå–å…³é”®è¯")
        
        processed = []
        for paper in papers:
            processed.append(self.process_paper(paper))
        
        total_keywords = sum(len(p.extracted_keywords) for p in processed)
        print(f"âœ… æå–å®Œæˆï¼Œå…± {total_keywords} ä¸ªå…³é”®è¯")
        
        return processed
    
    def _normalize_keywords(self, keywords: List[str]) -> List[str]:
        """
        æ ‡å‡†åŒ–å…³é”®è¯
        
        - è½¬æ¢ä¸ºå°å†™
        - å»é™¤é¦–å°¾ç©ºæ ¼
        - å»é‡
        - è¿‡æ»¤è¿‡çŸ­çš„å…³é”®è¯
        
        Args:
            keywords: åŸå§‹å…³é”®è¯åˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–åçš„å…³é”®è¯åˆ—è¡¨
        """
        normalized = set()
        for kw in keywords:
            kw = kw.strip().lower()
            # è¿‡æ»¤è¿‡çŸ­æˆ–è¿‡é•¿çš„å…³é”®è¯
            if 2 <= len(kw) <= 100:
                normalized.add(kw)
        return list(normalized)


def extract_keywords_batch(
    papers: List[Paper],
    extractor_type: ExtractorType = None,
    show_progress: bool = True,
) -> List[Paper]:
    """
    æ‰¹é‡æå–è®ºæ–‡å…³é”®è¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        extractor_type: æå–å™¨ç±»å‹
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
    Returns:
        å¤„ç†åçš„è®ºæ–‡åˆ—è¡¨
    """
    processor = KeywordProcessor(extractor_type=extractor_type)
    return processor.process_papers(papers, show_progress=show_progress)
