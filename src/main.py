"""
DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ

ä¸‰é˜¶æ®µå·¥ä½œæµæ¶æ„ï¼š
1. Ingestion Agent: é‡‡é›†åŸå§‹æ•°æ® â†’ Raw Layer
2. Structuring Agent: ç»“æ„åŒ–å¤„ç† â†’ Structured Layer
3. Analysis Agent: å…³é”®è¯æå–ä¸åˆ†æ â†’ Analysis Layer
"""

import argparse
import sys
from pathlib import Path
from typing import List, Optional
from datetime import datetime

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from agents import IngestionAgent, StructuringAgent
from agents.analysis_agent import AnalysisAgent
from database import get_repository
from analysis import get_analyzer
from visualization import generate_all_charts
from report import generate_report


def run_pipeline(
    sources: List[str] = None,
    arxiv_days: int = 7,
    venues: List[str] = None,
    years: List[int] = None,
    extractor: str = "yake",
    limit: int = 5000,
    skip_ingestion: bool = False,
    skip_structuring: bool = False,
) -> str:
    """
    è¿è¡Œä¸‰é˜¶æ®µå·¥ä½œæµ
    
    Args:
        sources: æ•°æ®æºåˆ—è¡¨ ["arxiv", "openalex", "s2", "openreview"]
        arxiv_days: arXiv é‡‡é›†å¤©æ•°
        venues: ä¼šè®®åˆ—è¡¨
        years: å¹´ä»½åˆ—è¡¨
        extractor: æå–å™¨ç±»å‹ ("yake", "keybert", "both")
        limit: æ¯é˜¶æ®µå¤„ç†ä¸Šé™
        skip_ingestion: è·³è¿‡é‡‡é›†é˜¶æ®µ
        skip_structuring: è·³è¿‡ç»“æ„åŒ–é˜¶æ®µ
        
    Returns:
        æŠ¥å‘Šè·¯å¾„
    """
    print("=" * 60)
    print("ğŸš€ DepthTrender - ä¸‰é˜¶æ®µå·¥ä½œæµ")
    print("=" * 60)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Stage 1: Ingestion (Raw Layer)
    if not skip_ingestion:
        print("\nğŸ“¥ é˜¶æ®µ 1/3: æ•°æ®é‡‡é›† (Ingestion)")
        print("-" * 40)
        
        ingestion_agent = IngestionAgent()
        ingestion_stats = ingestion_agent.run(
            sources=sources or ["arxiv", "openalex"],
            arxiv_days=arxiv_days,
            venues=venues,
            years=years,
        )
    else:
        print("\nâ­ï¸ è·³è¿‡é‡‡é›†é˜¶æ®µ")
    
    # Stage 2: Structuring (Structured Layer)
    if not skip_structuring:
        print("\nğŸ“ é˜¶æ®µ 2/3: æ•°æ®ç»“æ„åŒ– (Structuring)")
        print("-" * 40)
        
        structuring_agent = StructuringAgent()
        structuring_stats = structuring_agent.run()
    else:
        print("\nâ­ï¸ è·³è¿‡ç»“æ„åŒ–é˜¶æ®µ")
    
    # Stage 3: Analysis (Analysis Layer)
    print("\nğŸ”‘ é˜¶æ®µ 3/3: å…³é”®è¯åˆ†æ (Analysis)")
    print("-" * 40)
    
    analysis_agent = AnalysisAgent()
    
    # æ ¹æ® extractor å‚æ•°è¿è¡Œç›¸åº”çš„æå–å™¨
    if extractor == "yake":
        result_yake = analysis_agent.run(method="yake", limit=limit)
        print(f"   - YAKE: {result_yake['processed']} ç¯‡, {result_yake['keywords']} ä¸ªå…³é”®è¯")
    elif extractor == "keybert":
        result_kb = analysis_agent.run(method="keybert", limit=limit)
        print(f"   - KeyBERT: {result_kb['processed']} ç¯‡, {result_kb['keywords']} ä¸ªå…³é”®è¯")
    elif extractor == "both":
        result_yake = analysis_agent.run(method="yake", limit=limit)
        print(f"   - YAKE: {result_yake['processed']} ç¯‡, {result_yake['keywords']} ä¸ªå…³é”®è¯")
        result_kb = analysis_agent.run(method="keybert", limit=limit)
        print(f"   - KeyBERT: {result_kb['processed']} ç¯‡, {result_kb['keywords']} ä¸ªå…³é”®è¯")
    
    # è¿è¡Œç»Ÿè®¡åˆ†æ
    analyzer = get_analyzer()
    result = analyzer.analyze()
    
    print(f"\nâœ… åˆ†æå®Œæˆ")
    print(f"   - è®ºæ–‡æ€»æ•°: {result.total_papers:,}")
    print(f"   - å…³é”®è¯æ€»æ•°: {result.total_keywords:,}")
    if result.venues:
        print(f"   - è¦†ç›–ä¼šè®®: {', '.join(result.venues)}")
    if result.emerging_keywords:
        print(f"   - æ–°å…´å…³é”®è¯: {', '.join(result.emerging_keywords[:5])}...")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š")
    print("-" * 40)
    
    charts = generate_all_charts(result)
    report_path = generate_report(result, charts)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    return str(report_path)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´ä¸‰é˜¶æ®µå·¥ä½œæµ
  python src/main.py
  
  # ä»…é‡‡é›† arXiv æœ€è¿‘ 7 å¤©
  python src/main.py --source arxiv --arxiv-days 7
  
  # æŒ‡å®šä¼šè®®å’Œå¹´ä»½
  python src/main.py --venue ICLR NeurIPS --year 2024
  
  # è·³è¿‡é‡‡é›†ï¼Œåªè¿è¡Œç»“æ„åŒ–å’Œåˆ†æ
  python src/main.py --skip-ingestion
  
  # ä½¿ç”¨ KeyBERT æå–å™¨
  python src/main.py --extractor keybert
        """,
    )
    
    # æ•°æ®æºå‚æ•°
    parser.add_argument(
        "--source",
        type=str,
        choices=["arxiv", "openalex", "s2", "openreview", "all"],
        default="all",
        help="æ•°æ®æº (arxiv/openalex/s2/openreview/allï¼Œé»˜è®¤: all)",
    )
    
    parser.add_argument(
        "--arxiv-days",
        type=int,
        default=7,
        help="arXiv é‡‡é›†å¤©æ•°ï¼ˆé»˜è®¤: 7ï¼‰",
    )
    
    # ä¼šè®®å’Œå¹´ä»½
    parser.add_argument(
        "--venue",
        type=str,
        nargs="+",
        help="è¦å¤„ç†çš„ä¼šè®®ï¼ˆå¦‚: ICLR NeurIPS CVPRï¼‰",
    )
    
    parser.add_argument(
        "--year",
        type=int,
        nargs="+",
        help="è¦å¤„ç†çš„å¹´ä»½ï¼ˆå¦‚: 2024 2023ï¼‰",
    )
    
    # å¤„ç†é™åˆ¶
    parser.add_argument(
        "--limit",
        type=int,
        default=5000,
        help="æ¯é˜¶æ®µå¤„ç†ä¸Šé™ï¼ˆé»˜è®¤: 5000ï¼‰",
    )
    
    # æå–å™¨
    parser.add_argument(
        "--extractor",
        type=str,
        choices=["yake", "keybert", "both"],
        default="yake",
        help="å…³é”®è¯æå–å™¨ï¼ˆé»˜è®¤: yakeï¼‰",
    )
    
    # è·³è¿‡é€‰é¡¹
    parser.add_argument(
        "--skip-ingestion",
        action="store_true",
        help="è·³è¿‡é‡‡é›†é˜¶æ®µ",
    )
    
    parser.add_argument(
        "--skip-structuring",
        action="store_true",
        help="è·³è¿‡ç»“æ„åŒ–é˜¶æ®µ",
    )
    
    args = parser.parse_args()
    
    # è§£ææ•°æ®æº
    sources = None
    if args.source != "all":
        sources = [args.source]
    
    # è¿è¡Œä¸‰é˜¶æ®µå·¥ä½œæµ
    run_pipeline(
        sources=sources,
        arxiv_days=args.arxiv_days,
        venues=args.venue,
        years=args.year,
        extractor=args.extractor,
        limit=args.limit,
        skip_ingestion=args.skip_ingestion,
        skip_structuring=args.skip_structuring,
    )


if __name__ == "__main__":
    main()
