"""
DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ

ä¸‰é˜¶æ®µå·¥ä½œæµæ¶æ„ï¼š
1. Ingestion Agent: é‡‡é›†åŸå§‹æ•°æ® â†’ Raw Layer
2. Structuring Agent: ç»“æ„åŒ–å¤„ç† â†’ Structured Layer
3. Analysis Agent: å…³é”®è¯æå–ä¸åˆ†æ â†’ Analysis Layer
"""

import argparse
import sys
from pathlib import Path
from typing import List, Optional, Dict
from datetime import datetime

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from agents import IngestionAgent, StructuringAgent, run_ingestion, run_structuring
from scraper import scrape_all_venues, scrape_all_s2_venues, S2_VENUES
from scraper.models import Paper
from extractor import extract_keywords_batch
from database import get_repository, get_analysis_repository
from analysis import get_analyzer
from visualization import generate_all_charts
from report import generate_report
from config import VENUES


def run_new_pipeline(
    sources: List[str] = None,
    arxiv_days: int = 7,
    venues: List[str] = None,
    years: List[int] = None,
    extractor: str = "yake",
    skip_ingestion: bool = False,
    skip_structuring: bool = False,
) -> str:
    """
    è¿è¡Œæ–°çš„ä¸‰é˜¶æ®µæµç¨‹
    
    Args:
        sources: æ•°æ®æºåˆ—è¡¨ ["arxiv", "openalex", "s2", "openreview"]
        arxiv_days: arXiv é‡‡é›†å¤©æ•°
        venues: ä¼šè®®åˆ—è¡¨
        years: å¹´ä»½åˆ—è¡¨
        extractor: æå–å™¨ç±»å‹
        skip_ingestion: è·³è¿‡é‡‡é›†é˜¶æ®µ
        skip_structuring: è·³è¿‡ç»“æ„åŒ–é˜¶æ®µ
        
    Returns:
        æŠ¥å‘Šè·¯å¾„
    """
    print("=" * 60)
    print("ğŸš€ DepthTrender - ä¸‰é˜¶æ®µå·¥ä½œæµ")
    print("=" * 60)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Stage 1: Ingestion (Raw Layer)
    if not skip_ingestion:
        print("\nğŸ“¥ é˜¶æ®µ 1/3: æ•°æ®é‡‡é›† (Ingestion)")
        print("-" * 40)
        
        ingestion_agent = IngestionAgent()
        ingestion_stats = ingestion_agent.run(
            sources=sources or ["arxiv", "openalex"],
            arxiv_days=arxiv_days,
            venues=venues,
            years=years,
        )
    else:
        print("\nâ­ï¸ è·³è¿‡é‡‡é›†é˜¶æ®µ")
    
    # Stage 2: Structuring (Structured Layer)
    if not skip_structuring:
        print("\nğŸ“ é˜¶æ®µ 2/3: æ•°æ®ç»“æ„åŒ– (Structuring)")
        print("-" * 40)
        
        structuring_agent = StructuringAgent()
        structuring_stats = structuring_agent.run()
    else:
        print("\nâ­ï¸ è·³è¿‡ç»“æ„åŒ–é˜¶æ®µ")
    
    # Stage 3: Analysis (Analysis Layer)
    print("\nğŸ”‘ é˜¶æ®µ 3/3: å…³é”®è¯åˆ†æ (Analysis)")
    print("-" * 40)
    
    from agents.analysis_agent import AnalysisAgent
    
    # è¿è¡Œå…³é”®è¯æå– (YAKE)
    analysis_agent = AnalysisAgent()
    extraction_result = analysis_agent.run(method="yake", limit=5000)
    print(f"   - YAKE æå–: {extraction_result['processed']} ç¯‡, {extraction_result['keywords']} ä¸ªå…³é”®è¯")
    
    # è¿è¡Œç»Ÿè®¡åˆ†æ
    repo = get_repository()
    analyzer = get_analyzer()
    result = analyzer.analyze()
    
    print(f"âœ… åˆ†æå®Œæˆ")
    print(f"   - è®ºæ–‡æ€»æ•°: {result.total_papers:,}")
    print(f"   - å…³é”®è¯æ€»æ•°: {result.total_keywords:,}")
    if result.venues:
        print(f"   - è¦†ç›–ä¼šè®®: {', '.join(result.venues)}")
    if result.emerging_keywords:
        print(f"   - æ–°å…´å…³é”®è¯: {', '.join(result.emerging_keywords[:5])}...")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š")
    print("-" * 40)
    
    charts = generate_all_charts(result)
    report_path = generate_report(result, charts)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    return str(report_path)


def run_pipeline(
    venues: Optional[List[str]] = None,
    years: Optional[List[int]] = None,
    limit: Optional[int] = None,
    extractor: str = "yake",
    skip_scrape: bool = False,
    source: str = "all",
    max_age_days: int = 7,
) -> str:
    """
    è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    
    Args:
        venues: è¦å¤„ç†çš„ä¼šè®®åˆ—è¡¨
        years: è¦å¤„ç†çš„å¹´ä»½åˆ—è¡¨
        limit: æ¯ä¸ªä¼šè®®å¹´ä»½çš„è®ºæ–‡é™åˆ¶
        extractor: æå–å™¨ç±»å‹
        skip_scrape: è·³è¿‡çˆ¬å–
        source: æ•°æ®æº
        max_age_days: çˆ¬å–é—´éš”
        
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    print("=" * 60)
    print("ğŸš€ DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ")
    print("=" * 60)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    repo = get_repository()
    analyzer = get_analyzer()
    
    all_papers = []
    
    if not skip_scrape:
        print("\nğŸ“¥ æ­¥éª¤ 1/5: çˆ¬å–è®ºæ–‡")
        print("-" * 40)
        
        # OpenReview æ•°æ®æº
        if source in ("openreview", "all"):
            print("\nğŸ“š æ•°æ®æº: OpenReview")
            venue_configs = VENUES
            if venues:
                venue_configs = {k: v for k, v in VENUES.items() if k in venues}
            
            if venue_configs:
                or_papers = scrape_all_venues(
                    venues=venue_configs,
                    years=years,
                    limit_per_venue=limit,
                    max_age_days=max_age_days,
                    repository=repo,
                )
                all_papers.extend(or_papers)
        
        # Semantic Scholar æ•°æ®æº
        if source in ("s2", "all"):
            print("\nğŸ“š æ•°æ®æº: Semantic Scholar")
            s2_venues = S2_VENUES
            if venues:
                s2_venues = {k: v for k, v in S2_VENUES.items() if k in venues}
            
            if s2_venues:
                s2_papers = scrape_all_s2_venues(
                    venues=s2_venues,
                    years=years,
                    limit_per_venue=limit,
                    max_age_days=max_age_days,
                    repository=repo,
                )
                all_papers.extend(s2_papers)
        
        papers = all_papers
        
        if not papers:
            print("âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡")
            return None
        
        # æå–å…³é”®è¯
        print("\nğŸ”‘ æ­¥éª¤ 2/5: æå–å…³é”®è¯")
        print("-" * 40)
        
        papers = extract_keywords_batch(papers, extractor_type=extractor)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        print("\nğŸ’¾ æ­¥éª¤ 3/5: ä¿å­˜åˆ°æ•°æ®åº“")
        print("-" * 40)
        
        saved_count = repo.save_papers(papers)
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡è®ºæ–‡")
    else:
        print("\nâ­ï¸ è·³è¿‡çˆ¬å–ï¼Œä½¿ç”¨æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®")
    
    # ç»Ÿè®¡åˆ†æ
    print("\nğŸ“Š æ­¥éª¤ 4/5: ç»Ÿè®¡åˆ†æ")
    print("-" * 40)
    
    result = analyzer.analyze()
    print(f"âœ… åˆ†æå®Œæˆ")
    print(f"   - è®ºæ–‡æ€»æ•°: {result.total_papers:,}")
    print(f"   - å…³é”®è¯æ€»æ•°: {result.total_keywords:,}")
    if result.venues:
        print(f"   - è¦†ç›–ä¼šè®®: {', '.join(result.venues)}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ¨ æ­¥éª¤ 5/5: ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š")
    print("-" * 40)
    
    charts = generate_all_charts(result)
    report_path = generate_report(result, charts)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    return str(report_path)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ–°æ¶æ„ï¼šä¸‰é˜¶æ®µå·¥ä½œæµ
  python -m src.main --new-pipeline
  
  # æ–°æ¶æ„ï¼šä»…é‡‡é›† arXiv æœ€è¿‘ 7 å¤©
  python -m src.main --new-pipeline --source arxiv --arxiv-days 7
  
  # æ—§æ¥å£ï¼šè¿è¡Œå®Œæ•´æµç¨‹
  python src/main.py
  
  # æ—§æ¥å£ï¼šæŒ‡å®šä¼šè®®å’Œå¹´ä»½
  python src/main.py --venue ICLR --year 2024
        """,
    )
    
    # æ–°æ¶æ„å‚æ•°
    parser.add_argument(
        "--new-pipeline",
        action="store_true",
        help="ä½¿ç”¨æ–°çš„ä¸‰é˜¶æ®µå·¥ä½œæµ",
    )
    
    parser.add_argument(
        "--arxiv-days",
        type=int,
        default=7,
        help="arXiv é‡‡é›†å¤©æ•°ï¼ˆé»˜è®¤: 7ï¼‰",
    )
    
    parser.add_argument(
        "--skip-ingestion",
        action="store_true",
        help="è·³è¿‡é‡‡é›†é˜¶æ®µ",
    )
    
    parser.add_argument(
        "--skip-structuring",
        action="store_true",
        help="è·³è¿‡ç»“æ„åŒ–é˜¶æ®µ",
    )
    
    # åŸæœ‰å‚æ•°
    parser.add_argument(
        "--venue",
        type=str,
        nargs="+",
        help="è¦å¤„ç†çš„ä¼šè®®",
    )
    
    parser.add_argument(
        "--year",
        type=int,
        nargs="+",
        help="è¦å¤„ç†çš„å¹´ä»½",
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        help="æ¯ä¸ªä¼šè®®å¹´ä»½çš„è®ºæ–‡æ•°é‡é™åˆ¶",
    )
    
    parser.add_argument(
        "--extractor",
        type=str,
        choices=["yake", "keybert", "both"],
        default="yake",
        help="å…³é”®è¯æå–å™¨ï¼ˆé»˜è®¤: yakeï¼‰",
    )
    
    parser.add_argument(
        "--skip-scrape",
        action="store_true",
        help="è·³è¿‡çˆ¬å–",
    )
    
    parser.add_argument(
        "--source",
        type=str,
        choices=["openreview", "s2", "arxiv", "openalex", "all"],
        default="all",
        help="æ•°æ®æº",
    )
    
    parser.add_argument(
        "--max-age",
        type=int,
        default=7,
        help="çˆ¬å–é—´éš”å¤©æ•°",
    )
    
    args = parser.parse_args()
    
    if args.new_pipeline:
        # æ–°æ¶æ„
        sources = None
        if args.source != "all":
            sources = [args.source]
        
        run_new_pipeline(
            sources=sources,
            arxiv_days=args.arxiv_days,
            venues=args.venue,
            years=args.year,
            extractor=args.extractor,
            skip_ingestion=args.skip_ingestion,
            skip_structuring=args.skip_structuring,
        )
    else:
        # æ—§æ¥å£
        run_pipeline(
            venues=args.venue,
            years=args.year,
            limit=args.limit,
            extractor=args.extractor,
            skip_scrape=args.skip_scrape,
            source=args.source,
            max_age_days=args.max_age,
        )


if __name__ == "__main__":
    main()
