"""
DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ

ä¸»ç¨‹åºå…¥å£ï¼Œæä¾›å®Œæ•´çš„å·¥ä½œæµï¼š
1. çˆ¬å–è®ºæ–‡ï¼ˆæ”¯æŒ OpenReview å’Œ Semantic Scholarï¼‰
2. æå–å…³é”®è¯
3. å­˜å‚¨åˆ°æ•°æ®åº“
4. ç»Ÿè®¡åˆ†æ
5. ç”Ÿæˆå¯è§†åŒ–
6. ç”ŸæˆæŠ¥å‘Š
"""

import argparse
import sys
from pathlib import Path
from typing import List, Optional
from datetime import datetime

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from scraper import scrape_all_venues, scrape_venue
from scraper.semantic_scholar import scrape_all_s2_venues, S2_VENUES
from scraper.models import Paper
from extractor import extract_keywords_batch
from database import get_repository
from analysis import get_analyzer
from visualization import generate_all_charts
from report import generate_report
from config import VENUES, VenueConfig


def run_pipeline(
    venues: Optional[List[str]] = None,
    years: Optional[List[int]] = None,
    limit: Optional[int] = None,
    extractor: str = "yake",
    skip_scrape: bool = False,
    source: str = "all",  # "openreview", "s2", "all"
) -> str:
    """
    è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹
    
    Args:
        venues: è¦å¤„ç†çš„ä¼šè®®åˆ—è¡¨ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰
        years: è¦å¤„ç†çš„å¹´ä»½åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        limit: æ¯ä¸ªä¼šè®®å¹´ä»½çš„è®ºæ–‡é™åˆ¶
        extractor: æå–å™¨ç±»å‹ï¼ˆ"yake", "keybert", "both"ï¼‰
        skip_scrape: æ˜¯å¦è·³è¿‡çˆ¬å–ï¼ˆç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­çš„æ•°æ®ï¼‰
        source: æ•°æ®æºï¼ˆ"openreview", "s2", "all"ï¼‰
        
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    print("=" * 60)
    print("ğŸš€ DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ")
    print("=" * 60)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # åˆå§‹åŒ–ç»„ä»¶
    repo = get_repository()
    analyzer = get_analyzer()
    
    all_papers = []
    
    if not skip_scrape:
        # 1. çˆ¬å–è®ºæ–‡
        print("\nğŸ“¥ æ­¥éª¤ 1/5: çˆ¬å–è®ºæ–‡")
        print("-" * 40)
        
        # ========== OpenReview æ•°æ®æº ==========
        if source in ("openreview", "all"):
            print("\nğŸ“š æ•°æ®æº: OpenReview")
            venue_configs = VENUES
            if venues:
                venue_configs = {k: v for k, v in VENUES.items() if k in venues}
            
            if venue_configs:
                or_papers = scrape_all_venues(
                    venues=venue_configs,
                    years=years,
                    limit_per_venue=limit,
                )
                all_papers.extend(or_papers)
        
        # ========== Semantic Scholar æ•°æ®æº ==========
        if source in ("s2", "all"):
            print("\nğŸ“š æ•°æ®æº: Semantic Scholar")
            s2_venues = S2_VENUES
            if venues:
                s2_venues = {k: v for k, v in S2_VENUES.items() if k in venues}
            
            if s2_venues:
                s2_papers = scrape_all_s2_venues(
                    venues=s2_venues,
                    years=years,
                    limit_per_venue=limit,
                )
                all_papers.extend(s2_papers)
        
        papers = all_papers
        
        if not papers:
            print("âš ï¸ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¼šè®®é…ç½®")
            return None
        
        # 2. æå–å…³é”®è¯
        print("\nğŸ”‘ æ­¥éª¤ 2/5: æå–å…³é”®è¯")
        print("-" * 40)
        
        papers = extract_keywords_batch(papers, extractor_type=extractor)
        
        # 3. ä¿å­˜åˆ°æ•°æ®åº“
        print("\nğŸ’¾ æ­¥éª¤ 3/5: ä¿å­˜åˆ°æ•°æ®åº“")
        print("-" * 40)
        
        saved_count = repo.save_papers(papers)
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡è®ºæ–‡")
        
        # è®°å½•çˆ¬å–æ—¥å¿—
        for paper in papers:
            pass  # æ—¥å¿—å·²åœ¨ save_paper ä¸­å¤„ç†
    else:
        print("\nâ­ï¸ è·³è¿‡çˆ¬å–ï¼Œä½¿ç”¨æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®")
    
    # 4. ç»Ÿè®¡åˆ†æ
    print("\nğŸ“Š æ­¥éª¤ 4/5: ç»Ÿè®¡åˆ†æ")
    print("-" * 40)
    
    result = analyzer.analyze()
    print(f"âœ… åˆ†æå®Œæˆ")
    print(f"   - è®ºæ–‡æ€»æ•°: {result.total_papers:,}")
    print(f"   - å…³é”®è¯æ€»æ•°: {result.total_keywords:,}")
    print(f"   - è¦†ç›–ä¼šè®®: {', '.join(result.venues)}")
    
    # 5. ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ¨ æ­¥éª¤ 5/5: ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š")
    print("-" * 40)
    
    charts = generate_all_charts(result)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = generate_report(result, charts)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼è€—æ—¶: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    return str(report_path)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="DepthTrender - é¡¶ä¼šè®ºæ–‡å…³é”®è¯è¿½è¸ªç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆæ‰€æœ‰ä¼šè®®ï¼Œæ‰€æœ‰å¹´ä»½ï¼‰
  python -m src.main
  
  # åªå¤„ç† ICLR 2024
  python -m src.main --venue ICLR --year 2024
  
  # é™åˆ¶æ¯ä¸ªä¼šè®®å¹´ä»½åªå¤„ç† 10 ç¯‡è®ºæ–‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
  python -m src.main --limit 10
  
  # ä½¿ç”¨ KeyBERT æå–å™¨
  python -m src.main --extractor keybert
  
  # è·³è¿‡çˆ¬å–ï¼Œåªé‡æ–°ç”ŸæˆæŠ¥å‘Š
  python -m src.main --skip-scrape
        """,
    )
    
    parser.add_argument(
        "--venue",
        type=str,
        nargs="+",
        help="è¦å¤„ç†çš„ä¼šè®®ï¼ˆå¦‚ ICLR NeurIPS ICMLï¼‰",
    )
    
    parser.add_argument(
        "--year",
        type=int,
        nargs="+",
        help="è¦å¤„ç†çš„å¹´ä»½ï¼ˆå¦‚ 2024 2023ï¼‰",
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        help="æ¯ä¸ªä¼šè®®å¹´ä»½çš„è®ºæ–‡æ•°é‡é™åˆ¶",
    )
    
    parser.add_argument(
        "--extractor",
        type=str,
        choices=["yake", "keybert", "both"],
        default="yake",
        help="å…³é”®è¯æå–å™¨ï¼ˆé»˜è®¤: yakeï¼‰",
    )
    
    parser.add_argument(
        "--skip-scrape",
        action="store_true",
        help="è·³è¿‡çˆ¬å–ï¼Œä½¿ç”¨æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®",
    )
    
    parser.add_argument(
        "--source",
        type=str,
        choices=["openreview", "s2", "all"],
        default="all",
        help="æ•°æ®æºï¼ˆopenreview/s2/allï¼Œé»˜è®¤: allï¼‰",
    )
    
    args = parser.parse_args()
    
    run_pipeline(
        venues=args.venue,
        years=args.year,
        limit=args.limit,
        extractor=args.extractor,
        skip_scrape=args.skip_scrape,
        source=args.source,
    )


if __name__ == "__main__":
    main()
