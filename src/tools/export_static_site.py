#!/usr/bin/env python3
"""
é™æ€ç«™ç‚¹å¯¼å‡ºå·¥å…·

å°† DeepTrender çš„æ•°æ®å’Œé™æ€èµ„æºå¯¼å‡ºåˆ° docs/ ç›®å½•ï¼Œç”¨äº GitHub Pages éƒ¨ç½²ã€‚
"""

import sys
import json
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent.parent))

from database import get_repository
from config import VENUES, ROOT_DIR


class StaticSiteExporter:
    """é™æ€ç«™ç‚¹å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: str = "docs", top_keywords: int = 300):
        self.output_dir = Path(output_dir)
        self.data_dir = self.output_dir / "data"
        self.venues_data_dir = self.data_dir / "venues"
        self.arxiv_data_dir = self.data_dir / "arxiv"
        self.top_keywords = top_keywords
        
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.venues_data_dir.mkdir(parents=True, exist_ok=True)
        self.arxiv_data_dir.mkdir(parents=True, exist_ok=True)
        
        self.repo = get_repository()
        self.stats = {
            "venues_exported": 0,
            "arxiv_exported": 0,
            "files_copied": 0,
            "total_size_bytes": 0,
        }
    
    def export_venues_index(self) -> int:
        """å¯¼å‡ºä¼šè®®ç´¢å¼•"""
        print("\nğŸ“Š å¯¼å‡ºä¼šè®®ç´¢å¼•...")
        
        venues_data = []
        all_summaries = self.repo.analysis.get_all_venue_summaries()
        summary_map = {s["venue"]: s for s in all_summaries if s.get("year") is None}
        
        for venue_key, venue_config in VENUES.items():
            venue_name = venue_config.name
            summary = summary_map.get(venue_name)
            
            if summary:
                paper_count = summary.get("paper_count", 0)
                top_keywords = summary.get("top_keywords", [])[:10]
            else:
                paper_count = self.repo.get_paper_count(venue=venue_name)
                top_kw = self.repo.get_top_keywords(venue=venue_name, limit=10)
                top_keywords = [{"keyword": kw, "count": c} for kw, c in top_kw]
            
            years = self.repo.get_all_years(venue=venue_name)
            
            venue_data = {
                "name": venue_name,
                "full_name": venue_config.full_name,
                "domain": getattr(venue_config, "domain", "ML"),
                "tier": "A",
                "years_available": sorted(years, reverse=True),
                "paper_count": paper_count,
                "top_keywords": top_keywords,
            }
            venues_data.append(venue_data)
        
        output_file = self.venues_data_dir / "venues_index.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(venues_data, f, indent=2, ensure_ascii=False)
        
        self.stats["total_size_bytes"] += output_file.stat().st_size
        print(f"   âœ… å·²å¯¼å‡º {len(venues_data)} ä¸ªä¼šè®®")
        return len(venues_data)
    
    def export_venue_top_keywords(self, venue_name: str, top_n: int = 50) -> bool:
        """å¯¼å‡ºå•ä¸ªä¼šè®®çš„å¹´åº¦ top keywords"""
        years = self.repo.get_all_years(venue=venue_name)
        if not years:
            return False
        
        yearly_data = {}
        for year in sorted(years):
            top_keywords = self.repo.get_top_keywords(venue=venue_name, year=year, limit=top_n)
            yearly_data[str(year)] = [
                {"keyword": kw, "count": count, "rank": rank + 1}
                for rank, (kw, count) in enumerate(top_keywords)
            ]
        
        output_file = self.venues_data_dir / f"venue_{venue_name}_top_keywords.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(yearly_data, f, indent=2, ensure_ascii=False)
        
        self.stats["total_size_bytes"] += output_file.stat().st_size
        return True
    
    def export_venue_keyword_trends(self, venue_name: str, max_keywords: int = 300) -> bool:
        """å¯¼å‡ºå•ä¸ªä¼šè®®çš„å…³é”®è¯è¶‹åŠ¿"""
        years = self.repo.get_all_years(venue=venue_name)
        if not years:
            return False
        
        keyword_yearly_counts = defaultdict(dict)
        keyword_yearly_ranks = defaultdict(dict)
        
        for year in sorted(years):
            top_keywords = self.repo.get_top_keywords(venue=venue_name, year=year, limit=100)
            for rank, (kw, count) in enumerate(top_keywords, start=1):
                keyword_yearly_counts[kw][year] = count
                keyword_yearly_ranks[kw][year] = rank
        
        keyword_totals = {kw: sum(counts.values()) for kw, counts in keyword_yearly_counts.items()}
        top_keywords_list = sorted(keyword_totals.keys(), key=lambda k: keyword_totals[k], reverse=True)[:max_keywords]
        
        trends_data = {}
        for kw in top_keywords_list:
            yearly_data = []
            for year in sorted(years):
                count = keyword_yearly_counts[kw].get(year, 0)
                rank = keyword_yearly_ranks[kw].get(year, 0)
                yearly_data.append({"year": year, "count": count, "rank": rank})
            trends_data[kw] = yearly_data
        
        output_file = self.venues_data_dir / f"venue_{venue_name}_keyword_trends.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(trends_data, f, indent=2, ensure_ascii=False)
        
        self.stats["total_size_bytes"] += output_file.stat().st_size
        return True
    
    def export_venue_keywords_index(self, venue_name: str) -> bool:
        """å¯¼å‡ºä¼šè®®å…³é”®è¯ç´¢å¼•"""
        top_keywords = self.repo.get_top_keywords(venue=venue_name, limit=self.top_keywords)
        if not top_keywords:
            return False
        
        keywords_list = [kw for kw, _ in top_keywords]
        output_file = self.venues_data_dir / f"venue_{venue_name}_keywords_index.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(keywords_list, f, indent=2, ensure_ascii=False)
        
        self.stats["total_size_bytes"] += output_file.stat().st_size
        return True
    
    def export_all_venues(self) -> Dict:
        """å¯¼å‡ºæ‰€æœ‰ä¼šè®®æ•°æ®"""
        print("\nğŸ›ï¸ å¯¼å‡ºä¼šè®®æ•°æ®...")
        venues_count = self.export_venues_index()
        
        exported_venues = []
        for venue_key, venue_config in VENUES.items():
            venue_name = venue_config.name
            print(f"   ğŸ“„ å¯¼å‡º {venue_name}...")
            
            if self.export_venue_top_keywords(venue_name, top_n=50):
                if self.export_venue_keyword_trends(venue_name, max_keywords=self.top_keywords):
                    self.export_venue_keywords_index(venue_name)
                    exported_venues.append(venue_name)
        
        self.stats["venues_exported"] = len(exported_venues)
        print(f"\n   âœ… å·²å¯¼å‡º {len(exported_venues)} ä¸ªä¼šè®®çš„è¯¦ç»†æ•°æ®")
        return {"venues_count": venues_count, "venues_exported": exported_venues}
    
    def export_arxiv_timeseries(self) -> int:
        print("\nğŸ“ˆ å¯¼å‡º arXiv æ—¶é—´åºåˆ—...")
        
        granularities = ["day", "week", "month", "year"]
        categories = ["ALL", "cs.LG", "cs.CV", "cs.CL", "cs.AI", "cs.RO"]
        exported_count = 0
        
        for granularity in granularities:
            for category in categories:
                try:
                    data = self.repo.analysis.get_arxiv_timeseries(category, granularity)
                    if not data:
                        continue
                    
                    output_data = {
                        "granularity": granularity,
                        "category": category,
                        "data": data,
                        "cached": True,
                        "exported_at": datetime.now().isoformat()
                    }
                    
                    filename = f"arxiv_timeseries_{granularity}_{category}.json"
                    output_file = self.arxiv_data_dir / filename
                    
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(output_data, f, indent=2, ensure_ascii=False)
                    
                    self.stats["total_size_bytes"] += output_file.stat().st_size
                    exported_count += 1
                except Exception as e:
                    print(f"   âš ï¸  è·³è¿‡ {granularity}/{category}: {e}")
        
        print(f"   âœ… å·²å¯¼å‡º {exported_count} ä¸ªæ—¶é—´åºåˆ—æ–‡ä»¶")
        return exported_count
    
    def export_arxiv_emerging(self) -> int:
        print("\nğŸš€ å¯¼å‡º arXiv æ–°å…´å…³é”®è¯...")
        
        categories = ["ALL", "cs.LG", "cs.CV", "cs.CL", "cs.AI", "cs.RO"]
        exported_count = 0
        
        for category in categories:
            try:
                topics = self.repo.analysis.get_emerging_topics(
                    category=category,
                    limit=50,
                    min_growth_rate=1.5
                )
                
                if not topics:
                    continue
                
                filename = f"arxiv_emerging_{category}.json"
                output_file = self.arxiv_data_dir / filename
                
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(topics, f, indent=2, ensure_ascii=False)
                
                self.stats["total_size_bytes"] += output_file.stat().st_size
                exported_count += 1
            except Exception as e:
                print(f"   âš ï¸  è·³è¿‡ {category}: {e}")
        
        print(f"   âœ… å·²å¯¼å‡º {exported_count} ä¸ªæ–°å…´å…³é”®è¯æ–‡ä»¶")
        return exported_count
    
    def export_arxiv_stats(self) -> bool:
        print("\nğŸ“Š å¯¼å‡º arXiv ç»Ÿè®¡...")
        
        try:
            total_papers = self.repo.raw.get_raw_paper_count(source="arxiv")
            
            categories_stats = {}
            for category in ["cs.LG", "cs.CL", "cs.CV", "cs.AI", "cs.RO"]:
                with self.repo._get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT COUNT(*) as count
                        FROM raw_papers
                        WHERE source = 'arxiv' AND categories LIKE ?
                    """, (f"%{category}%",))
                    count = cursor.fetchone()["count"]
                    categories_stats[category] = count
            
            with self.repo._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT MIN(retrieved_at) as min_date, MAX(retrieved_at) as max_date
                    FROM raw_papers
                    WHERE source = 'arxiv'
                """)
                row = cursor.fetchone()
                date_range = {
                    "min": row["min_date"] if row["min_date"] else None,
                    "max": row["max_date"] if row["max_date"] else None
                }
            
            latest_update = self.repo.analysis.get_meta("arxiv_last_run_ALL_year")
            
            stats_data = {
                "total_papers": total_papers,
                "categories": categories_stats,
                "date_range": date_range,
                "latest_update": latest_update,
                "exported_at": datetime.now().isoformat()
            }
            
            output_file = self.arxiv_data_dir / "arxiv_stats.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(stats_data, f, indent=2, ensure_ascii=False)
            
            self.stats["total_size_bytes"] += output_file.stat().st_size
            print("   âœ… å·²å¯¼å‡º arXiv ç»Ÿè®¡")
            return True
        except Exception as e:
            print(f"   âš ï¸  å¯¼å‡ºå¤±è´¥: {e}")
            return False
    
    def export_all_arxiv(self) -> Dict:
        print("\nğŸ“¡ å¯¼å‡º arXiv æ•°æ®...")
        
        results = {
            "timeseries": self.export_arxiv_timeseries(),
            "emerging": self.export_arxiv_emerging(),
            "stats": self.export_arxiv_stats(),
        }
        
        self.stats["arxiv_exported"] = sum(1 for v in results.values() if v)
        return results
    
    def copy_static_assets(self) -> int:
        """å¤åˆ¶é™æ€èµ„æº"""
        print("\nğŸ“¦ å¤åˆ¶é™æ€èµ„æº...")
        src_static = ROOT_DIR / "src" / "web" / "static"
        
        if not src_static.exists():
            print("   âš ï¸  é™æ€èµ„æºç›®å½•ä¸å­˜åœ¨")
            return 0
        
        copied_count = 0
        for item in src_static.rglob("*"):
            if item.is_file():
                rel_path = item.relative_to(src_static)
                dest_path = self.output_dir / rel_path
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(item, dest_path)
                copied_count += 1
                self.stats["total_size_bytes"] += dest_path.stat().st_size
        
        self.stats["files_copied"] = copied_count
        print(f"   âœ… å·²å¤åˆ¶ {copied_count} ä¸ªæ–‡ä»¶")
        return copied_count
    
    def transform_paths_in_html(self) -> int:
        """è½¬æ¢ HTML è·¯å¾„"""
        print("\nğŸ”§ è½¬æ¢ HTML è·¯å¾„...")
        html_files = list(self.output_dir.glob("*.html"))
        processed_count = 0
        
        for html_file in html_files:
            content = html_file.read_text(encoding="utf-8")
            original = content
            content = content.replace('href="/static/', 'href="./static/')
            content = content.replace('src="/static/', 'src="./static/')
            
            if content != original:
                html_file.write_text(content, encoding="utf-8")
                processed_count += 1
        
        print(f"   âœ… å·²å¤„ç† {processed_count} ä¸ª HTML æ–‡ä»¶")
        return processed_count
    
    def export_all(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´å¯¼å‡º"""
        start_time = datetime.now()
        print("=" * 60)
        print("ğŸš€ å¼€å§‹å¯¼å‡ºé™æ€ç«™ç‚¹")
        print("=" * 60)
        
        venues_result = self.export_all_venues()
        arxiv_result = self.export_all_arxiv()
        self.copy_static_assets()
        self.transform_paths_in_html()
        
        duration = (datetime.now() - start_time).total_seconds()
        
        summary = {
            **self.stats,
            "output_dir": str(self.output_dir.absolute()),
            "total_size_mb": self.stats["total_size_bytes"] / 1024 / 1024,
            "export_time": f"{duration:.2f}s",
            "venues": venues_result,
            "arxiv": arxiv_result,
        }
        
        print("\n" + "=" * 60)
        print("âœ… å¯¼å‡ºå®Œæˆ")
        print("=" * 60)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {summary['output_dir']}")
        print(f"ğŸ›ï¸  ä¼šè®®æ•°æ®: {summary['venues_exported']} ä¸ªä¼šè®®")
        print(f"ğŸ“¡ arXiv æ•°æ®: {summary['arxiv_exported']} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“¦ é™æ€èµ„æº: {summary['files_copied']} ä¸ªæ–‡ä»¶")
        print(f"ğŸ’¾ æ€»å¤§å°: {summary['total_size_mb']:.2f} MB")
        print(f"â±ï¸  è€—æ—¶: {summary['export_time']}")
        print("=" * 60)
        
        summary_file = self.output_dir / "export_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        return summary


def main():
    parser = argparse.ArgumentParser(description="å¯¼å‡º DeepTrender é™æ€ç«™ç‚¹")
    parser.add_argument("--output-dir", default="docs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--top-keywords", type=int, default=300, help="æœ€å¤§å…³é”®è¯æ•°")
    args = parser.parse_args()
    
    exporter = StaticSiteExporter(output_dir=args.output_dir, top_keywords=args.top_keywords)
    
    try:
        exporter.export_all()
        print(f"\nğŸ’¡ æœ¬åœ°æµ‹è¯•: python -m http.server -d {args.output_dir} 8000")
        return 0
    except Exception as e:
        print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
