"""
arXiv API å®¢æˆ·ç«¯

ä½œä¸º Raw Layer çš„ä¸»è¦æ•°æ®æºï¼Œç”¨äºå¤§è§„æ¨¡è®ºæ–‡é‡‡é›†ã€‚
arXiv æä¾›æœ€å¹¿æ³›çš„ AI ç›¸å…³è®ºæ–‡è¦†ç›–ï¼Œæ›´æ–°æœ€å¿«ã€‚

ä½¿ç”¨è¯´æ˜:
- æŒ‰é¢†åŸŸé‡‡é›†ï¼ˆcs.CV / cs.CL / cs.LG / cs.AIï¼‰
- ä¸åšä¼šè®®ç­›é€‰ï¼ˆä»…ç”¨äº Raw Layerï¼‰
- å®Œæ•´ä¿å­˜æ‰€æœ‰å…ƒæ•°æ®
"""

import time
import urllib.parse
import feedparser
import requests
from typing import List, Optional, Iterator, Dict, Any
from datetime import datetime, timedelta
from dataclasses import dataclass

from .models import RawPaper


# arXiv API é…ç½®
ARXIV_API_URL = "http://export.arxiv.org/api/query"
ARXIV_BULK_URL = "https://arxiv.org/list"

# é»˜è®¤ AI ç›¸å…³ç±»åˆ«
DEFAULT_CATEGORIES = ["cs.CV", "cs.CL", "cs.LG", "cs.AI", "cs.RO", "cs.NE", "stat.ML"]


@dataclass
class ArxivQuery:
    """arXiv æŸ¥è¯¢å‚æ•°"""
    categories: List[str] = None  # cs.CV, cs.LG, etc.
    search_query: str = None  # è‡ªå®šä¹‰æœç´¢è¯
    start_date: datetime = None
    end_date: datetime = None
    max_results: int = 1000
    

class ArxivClient:
    """arXiv API å®¢æˆ·ç«¯"""
    
    def __init__(self, delay: float = 3.0):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼ŒarXiv è¦æ±‚è‡³å°‘ 3 ç§’
        """
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "DepthTrender/1.0 (https://github.com/depthtrender)"
        })
        self._last_request = 0
    
    def _wait_for_rate_limit(self):
        """éµå®ˆ arXiv é€Ÿç‡é™åˆ¶"""
        elapsed = time.time() - self._last_request
        if elapsed < self.delay:
            time.sleep(self.delay - elapsed)
        self._last_request = time.time()
    
    def search(
        self,
        categories: List[str] = None,
        search_query: str = None,
        start: int = 0,
        max_results: int = 100,
    ) -> List[RawPaper]:
        """
        æœç´¢ arXiv è®ºæ–‡
        
        Args:
            categories: arXiv ç±»åˆ«åˆ—è¡¨ï¼ˆå¦‚ ["cs.CV", "cs.LG"]ï¼‰
            search_query: æœç´¢è¯
            start: èµ·å§‹ä½ç½®
            max_results: æœ€å¤§ç»“æœæ•°ï¼ˆå•æ¬¡æœ€å¤š 2000ï¼‰
            
        Returns:
            RawPaper åˆ—è¡¨
        """
        self._wait_for_rate_limit()
        
        # æ„å»ºæŸ¥è¯¢
        query_parts = []
        
        if categories:
            cat_query = " OR ".join([f"cat:{cat}" for cat in categories])
            query_parts.append(f"({cat_query})")
        
        if search_query:
            query_parts.append(f"({search_query})")
        
        query = " AND ".join(query_parts) if query_parts else "cat:cs.LG"
        
        params = {
            "search_query": query,
            "start": start,
            "max_results": min(max_results, 2000),
            "sortBy": "submittedDate",
            "sortOrder": "descending",
        }
        
        try:
            response = self.session.get(ARXIV_API_URL, params=params)
            response.raise_for_status()
            
            # è§£æ Atom feed
            feed = feedparser.parse(response.text)
            papers = []
            
            for entry in feed.entries:
                paper = self._parse_entry(entry)
                if paper:
                    papers.append(paper)
            
            return papers
            
        except Exception as e:
            print(f"arXiv API è¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def search_recent(
        self,
        categories: List[str] = None,
        days: int = 7,
        max_results: int = 1000,
    ) -> List[RawPaper]:
        """
        è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡
        
        Args:
            categories: arXiv ç±»åˆ«
            days: å¤©æ•°
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            RawPaper åˆ—è¡¨
        """
        categories = categories or DEFAULT_CATEGORIES
        
        print(f"ğŸ” æ­£åœ¨ä» arXiv è·å–æœ€è¿‘ {days} å¤©çš„è®ºæ–‡...")
        print(f"   ç±»åˆ«: {', '.join(categories)}")
        
        all_papers = []
        start = 0
        batch_size = 500
        
        while len(all_papers) < max_results:
            papers = self.search(
                categories=categories,
                start=start,
                max_results=min(batch_size, max_results - len(all_papers)),
            )
            
            if not papers:
                break
            
            # è¿‡æ»¤æ—¥æœŸ
            cutoff_date = datetime.now() - timedelta(days=days)
            recent_papers = []
            
            for paper in papers:
                if paper.retrieved_at and paper.retrieved_at >= cutoff_date:
                    recent_papers.append(paper)
                elif paper.year and paper.year >= cutoff_date.year:
                    recent_papers.append(paper)
            
            all_papers.extend(recent_papers)
            
            # å¦‚æœè¿™æ‰¹æ¬¡éƒ½å¤ªæ—§äº†ï¼Œåœæ­¢
            if len(recent_papers) < len(papers) * 0.5:
                break
            
            start += batch_size
            print(f"   å·²è·å– {len(all_papers)} ç¯‡è®ºæ–‡...")
        
        print(f"âœ… arXiv: å…±è·å– {len(all_papers)} ç¯‡è®ºæ–‡")
        return all_papers
    
    def search_by_category(
        self,
        category: str,
        max_results: int = 1000,
    ) -> List[RawPaper]:
        """
        æŒ‰ç±»åˆ«è·å–è®ºæ–‡
        
        Args:
            category: arXiv ç±»åˆ«ï¼ˆå¦‚ "cs.CV"ï¼‰
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            RawPaper åˆ—è¡¨
        """
        print(f"ğŸ” æ­£åœ¨ä» arXiv è·å– {category} ç±»åˆ«è®ºæ–‡...")
        
        all_papers = []
        start = 0
        batch_size = 500
        
        while len(all_papers) < max_results:
            papers = self.search(
                categories=[category],
                start=start,
                max_results=min(batch_size, max_results - len(all_papers)),
            )
            
            if not papers:
                break
            
            all_papers.extend(papers)
            start += batch_size
            
            print(f"   å·²è·å– {len(all_papers)} ç¯‡è®ºæ–‡...")
        
        print(f"âœ… arXiv {category}: å…±è·å– {len(all_papers)} ç¯‡è®ºæ–‡")
        return all_papers
    
    def get_paper(self, arxiv_id: str) -> Optional[RawPaper]:
        """
        è·å–å•ç¯‡è®ºæ–‡
        
        Args:
            arxiv_id: arXiv IDï¼ˆå¦‚ "2312.12345"ï¼‰
            
        Returns:
            RawPaper
        """
        self._wait_for_rate_limit()
        
        params = {
            "id_list": arxiv_id,
            "max_results": 1,
        }
        
        try:
            response = self.session.get(ARXIV_API_URL, params=params)
            response.raise_for_status()
            
            feed = feedparser.parse(response.text)
            if feed.entries:
                return self._parse_entry(feed.entries[0])
            return None
            
        except Exception as e:
            print(f"è·å– arXiv è®ºæ–‡å¤±è´¥: {e}")
            return None
    
    def _parse_entry(self, entry: Dict[str, Any]) -> Optional[RawPaper]:
        """è§£æ arXiv Atom entry ä¸º RawPaper"""
        try:
            # æå– arXiv ID
            arxiv_id = entry.get("id", "").split("/abs/")[-1].split("v")[0]
            if not arxiv_id:
                return None
            
            # æ ‡é¢˜ï¼ˆç§»é™¤æ¢è¡Œï¼‰
            title = entry.get("title", "").replace("\n", " ").strip()
            
            # æ‘˜è¦
            abstract = entry.get("summary", "").replace("\n", " ").strip()
            
            # ä½œè€…
            authors = []
            for author in entry.get("authors", []):
                name = author.get("name", "")
                if name:
                    authors.append(name)
            
            # å‘å¸ƒæ—¥æœŸ
            published = entry.get("published", "")
            year = None
            if published:
                try:
                    year = int(published[:4])
                except:
                    pass
            
            # ç±»åˆ«
            categories = []
            for tag in entry.get("tags", []):
                term = tag.get("term", "")
                if term:
                    categories.append(term)
            
            # Commentsï¼ˆå¯èƒ½åŒ…å«ä¼šè®®ä¿¡æ¯ï¼‰
            comments = entry.get("arxiv_comment", "")
            
            # Journal reference
            journal_ref = entry.get("arxiv_journal_ref", "")
            
            # DOI
            doi = entry.get("arxiv_doi", "")
            
            # PDF é“¾æ¥
            pdf_url = None
            for link in entry.get("links", []):
                if link.get("type") == "application/pdf":
                    pdf_url = link.get("href")
                    break
            
            return RawPaper(
                source="arxiv",
                source_paper_id=arxiv_id,
                title=title,
                abstract=abstract,
                authors=authors,
                year=year,
                venue_raw=None,  # arXiv æœ¬èº«ä¸æ˜¯ venue
                journal_ref=journal_ref,
                comments=comments,
                categories=",".join(categories),
                doi=doi,
                raw_json={
                    "id": entry.get("id"),
                    "published": published,
                    "updated": entry.get("updated"),
                    "pdf_url": pdf_url,
                    "links": [l.get("href") for l in entry.get("links", [])],
                },
                retrieved_at=datetime.now(),
            )
            
        except Exception as e:
            print(f"è§£æ arXiv entry å¤±è´¥: {e}")
            return None


def create_arxiv_client(delay: float = 3.0) -> ArxivClient:
    """åˆ›å»º arXiv å®¢æˆ·ç«¯"""
    return ArxivClient(delay=delay)
