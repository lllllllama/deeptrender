"""
Semantic Scholar API å®¢æˆ·ç«¯

ç”¨äºè·å– CVPRã€ACLã€AAAI ç­‰ä¸åœ¨ OpenReview ä¸Šçš„ä¼šè®®è®ºæ–‡ã€‚
Semantic Scholar æ˜¯ä¸€ä¸ªå…è´¹çš„å­¦æœ¯è®ºæ–‡æœç´¢å¼•æ“ï¼Œæä¾› API è®¿é—®ã€‚
"""

import time
import requests
from typing import List, Optional, Iterator, Dict, Any
from dataclasses import dataclass

from .models import Paper


# Semantic Scholar API é…ç½®
S2_API_URL = "https://api.semanticscholar.org/graph/v1"
S2_SEARCH_URL = f"{S2_API_URL}/paper/search/bulk"
S2_PAPER_URL = f"{S2_API_URL}/paper"

# è¯·æ±‚å­—æ®µ
S2_FIELDS = [
    "paperId", "title", "abstract", "authors", "venue", 
    "year", "url", "externalIds", "publicationTypes"
]


@dataclass
class SemanticScholarConfig:
    """Semantic Scholar ä¼šè®®é…ç½®"""
    name: str  # ä¼šè®®ç®€ç§°
    full_name: str  # ä¼šè®®å…¨ç§°
    venue_query: str  # ç”¨äºæœç´¢çš„ venue å…³é”®è¯
    years: List[int]


# æ”¯æŒçš„ä¼šè®®ï¼ˆé€šè¿‡ Semantic Scholarï¼‰
S2_VENUES: Dict[str, SemanticScholarConfig] = {
    # ========== è®¡ç®—æœºè§†è§‰ä¼šè®® ==========
    "CVPR": SemanticScholarConfig(
        name="CVPR",
        full_name="IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        venue_query="CVPR",
        years=[2024, 2023, 2022, 2021]
    ),
    "ICCV": SemanticScholarConfig(
        name="ICCV",
        full_name="IEEE/CVF International Conference on Computer Vision",
        venue_query="ICCV",
        years=[2023, 2021]  # æ¯ä¸¤å¹´ä¸€æ¬¡
    ),
    "ECCV": SemanticScholarConfig(
        name="ECCV",
        full_name="European Conference on Computer Vision",
        venue_query="ECCV",
        years=[2024, 2022]  # æ¯ä¸¤å¹´ä¸€æ¬¡
    ),
    
    # ========== è‡ªç„¶è¯­è¨€å¤„ç†ä¼šè®® ==========
    "ACL": SemanticScholarConfig(
        name="ACL",
        full_name="Annual Meeting of the Association for Computational Linguistics",
        venue_query="ACL",
        years=[2024, 2023, 2022]
    ),
    "NAACL": SemanticScholarConfig(
        name="NAACL",
        full_name="North American Chapter of the ACL",
        venue_query="NAACL",
        years=[2024, 2022]
    ),
    
    # ========== äººå·¥æ™ºèƒ½ç»¼åˆä¼šè®® ==========
    "AAAI": SemanticScholarConfig(
        name="AAAI",
        full_name="AAAI Conference on Artificial Intelligence",
        venue_query="AAAI",
        years=[2024, 2023, 2022]
    ),
    "IJCAI": SemanticScholarConfig(
        name="IJCAI",
        full_name="International Joint Conference on Artificial Intelligence",
        venue_query="IJCAI",
        years=[2024, 2023, 2022]
    ),
}


class SemanticScholarClient:
    """Semantic Scholar API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            api_key: API Keyï¼ˆå¯é€‰ï¼Œç”¨äºæé«˜é€Ÿç‡é™åˆ¶ï¼‰
        """
        self.api_key = api_key
        self.session = requests.Session()
        if api_key:
            self.session.headers["x-api-key"] = api_key
        
        # é€Ÿç‡é™åˆ¶ï¼šæ—  key æ—¶ 100 req/5min
        self.request_delay = 0.5
    
    def search_papers(
        self,
        venue: str,
        year: int,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢æŒ‡å®šä¼šè®®å’Œå¹´ä»½çš„è®ºæ–‡
        
        Args:
            venue: ä¼šè®®åç§°ï¼ˆå¦‚ "CVPR"ï¼‰
            year: å¹´ä»½
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        papers = []
        token = None
        
        while True:
            params = {
                "query": "",
                "venue": venue,
                "year": str(year),
                "fields": ",".join(S2_FIELDS),
                "limit": min(limit or 1000, 1000),
            }
            
            if token:
                params["token"] = token
            
            try:
                response = self.session.get(S2_SEARCH_URL, params=params)
                response.raise_for_status()
                data = response.json()
                
                batch = data.get("data", [])
                papers.extend(batch)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µ
                token = data.get("token")
                if not token or (limit and len(papers) >= limit):
                    break
                
                time.sleep(self.request_delay)
                
            except requests.RequestException as e:
                print(f"Semantic Scholar API è¯·æ±‚å¤±è´¥: {e}")
                break
        
        if limit:
            papers = papers[:limit]
        
        return papers
    
    def get_paper_by_id(self, paper_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å•ç¯‡è®ºæ–‡è¯¦æƒ…"""
        try:
            url = f"{S2_PAPER_URL}/{paper_id}"
            params = {"fields": ",".join(S2_FIELDS)}
            response = self.session.get(url, params=params)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"è·å–è®ºæ–‡å¤±è´¥: {e}")
            return None


def parse_s2_paper(data: Dict[str, Any], venue: str, year: int) -> Optional[Paper]:
    """
    å°† Semantic Scholar æ•°æ®è½¬æ¢ä¸º Paper å¯¹è±¡
    
    Args:
        data: Semantic Scholar è®ºæ–‡æ•°æ®
        venue: ä¼šè®®åç§°
        year: å¹´ä»½
        
    Returns:
        Paper å¯¹è±¡
    """
    try:
        paper_id = data.get("paperId", "")
        title = data.get("title", "")
        abstract = data.get("abstract", "")
        
        # æå–ä½œè€…å
        authors = []
        for author in data.get("authors", []):
            if isinstance(author, dict):
                name = author.get("name", "")
                if name:
                    authors.append(name)
        
        # URL
        url = data.get("url", f"https://www.semanticscholar.org/paper/{paper_id}")
        
        return Paper(
            id=f"s2:{paper_id}",  # æ·»åŠ å‰ç¼€åŒºåˆ†æ¥æº
            title=title,
            abstract=abstract or "",
            authors=authors,
            venue=venue,
            year=year,
            url=url,
            keywords=[],  # Semantic Scholar ä¸ç›´æ¥æä¾›å…³é”®è¯
        )
    except Exception as e:
        print(f"è§£æè®ºæ–‡å¤±è´¥: {e}")
        return None


def scrape_s2_venue(
    config: SemanticScholarConfig,
    year: int,
    client: Optional[SemanticScholarClient] = None,
    limit: Optional[int] = None,
) -> List[Paper]:
    """
    çˆ¬å–æŒ‡å®šä¼šè®®çš„è®ºæ–‡ï¼ˆé€šè¿‡ Semantic Scholarï¼‰
    
    Args:
        config: ä¼šè®®é…ç½®
        year: å¹´ä»½
        client: S2 å®¢æˆ·ç«¯
        limit: è®ºæ–‡æ•°é‡é™åˆ¶
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    if client is None:
        client = SemanticScholarClient()
    
    print(f"\nğŸ” æ­£åœ¨ä» Semantic Scholar è·å– {config.name} {year}...")
    
    raw_papers = client.search_papers(config.venue_query, year, limit)
    
    papers = []
    for data in raw_papers:
        paper = parse_s2_paper(data, config.name, year)
        if paper and paper.title:  # è¿‡æ»¤æ— æ•ˆè®ºæ–‡
            papers.append(paper)
    
    print(f"âœ… {config.name} {year}: è·å– {len(papers)} ç¯‡è®ºæ–‡")
    return papers


def scrape_all_s2_venues(
    venues: Optional[Dict[str, SemanticScholarConfig]] = None,
    years: Optional[List[int]] = None,
    limit_per_venue: Optional[int] = None,
    max_age_days: int = 7,
    repository = None,
) -> List[Paper]:
    """
    çˆ¬å–æ‰€æœ‰ Semantic Scholar ä¼šè®®
    
    Args:
        venues: ä¼šè®®é…ç½®
        years: å¹´ä»½åˆ—è¡¨
        limit_per_venue: æ¯ä¸ªä¼šè®®çš„è®ºæ–‡é™åˆ¶
        max_age_days: æœ€å¤§çˆ¬å–é—´éš”å¤©æ•°ï¼Œåœ¨æ­¤æ—¶é—´å†…çˆ¬å–è¿‡çš„ä¼šè®®å°†è¢«è·³è¿‡ï¼ˆé»˜è®¤ 7 å¤©ï¼‰
        repository: æ•°æ®åº“ä»“åº“ï¼ˆç”¨äºæ£€æŸ¥å’Œè®°å½•çˆ¬å–æ—¥å¿—ï¼‰
        
    Returns:
        æ‰€æœ‰è®ºæ–‡åˆ—è¡¨
    """
    if venues is None:
        venues = S2_VENUES
    
    client = SemanticScholarClient()
    all_papers = []
    skipped_count = 0
    
    for venue_name, config in venues.items():
        venue_years = years if years is not None else config.years
        
        for year in venue_years:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
            if repository is not None and not repository.should_scrape(config.name, year, max_age_days):
                print(f"â­ï¸ è·³è¿‡ {config.name} {year}ï¼ˆ{max_age_days} å¤©å†…å·²çˆ¬å–ï¼‰")
                skipped_count += 1
                continue
            
            try:
                papers = scrape_s2_venue(config, year, client, limit_per_venue)
                all_papers.extend(papers)
                
                # è®°å½•çˆ¬å–æ—¥å¿—
                if repository is not None and papers:
                    repository.log_scrape(config.name, year, len(papers))
                    
            except Exception as e:
                print(f"âŒ çˆ¬å– {venue_name} {year} å¤±è´¥: {e}")
                continue
    
    print(f"\nğŸ“Š Semantic Scholar æ€»è®¡è·å– {len(all_papers)} ç¯‡è®ºæ–‡ï¼ˆè·³è¿‡ {skipped_count} ä¸ªä¼šè®®å¹´ä»½ï¼‰")
    return all_papers

